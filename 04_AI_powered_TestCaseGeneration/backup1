import json
import pandas as pd
from typing import List, Dict, Any, Optional
from dataclasses import dataclass, asdict
from datetime import datetime
from enum import Enum
import ollama
from concurrent.futures import ThreadPoolExecutor, as_completed
import time


# ============================================
# ENHANCED DATA STRUCTURES FOR HYBRID MODE
# ============================================

class TestCaseType(Enum):
    """Complete test case types"""
    # Essential (Phase 1 - Fast Batch)
    POSITIVE = "positive"
    NEGATIVE = "negative"
    EDGE = "edge"
    INTEGRATION = "integration"
    PERFORMANCE = "performance"
    SECURITY = "security"
    
    # Comprehensive (Phase 2 - Critical Requirements)
    USABILITY = "usability"
    COMPATIBILITY = "compatibility"
    API = "api"
    DATA_INTEGRITY = "data_integrity"
    RELIABILITY = "reliability"


@dataclass
class TestCase:
    """Enhanced test case structure"""
    test_id: str
    requirement_id: str
    test_type: str
    test_title: str
    preconditions: str
    test_steps: List[str]
    expected_result: str
    test_data: str
    priority: str
    generation_phase: str  # "fast_batch" or "comprehensive"


# ============================================
# COMPREHENSIVE PROMPT GENERATOR (Phase 2)
# ============================================

class ComprehensivePromptGenerator:
    """Deep prompt generator with few-shot examples for critical requirements"""
    
    @staticmethod
    def build_deep_prompt(requirement: Dict, domain: str, test_type: str) -> str:
        """Generate detailed prompt for single test type with examples"""
        
        few_shot_examples = {
            "usability": """
Example Usability Test:
{
  "test_type": "usability",
  "test_title": "Verify location search filter usability and intuitiveness",
  "preconditions": "App launched, User on search screen, Multiple filters available",
  "test_steps": [
    "Observe filter panel layout and organization",
    "Test filter selection with single tap",
    "Apply multiple filters simultaneously",
    "Verify filter clear functionality",
    "Measure time to complete filter selection (target: <10 seconds)",
    "Test filter visibility and readability on different screen sizes"
  ],
  "expected_result": "Filters are intuitive, clearly labeled, easy to access. Users can apply filters in under 10 seconds. Filter state is clearly visible. Clear all filters option is prominent.",
  "test_data": "Filter options: Cuisine type, Distance, Rating, Price range, Open now",
  "priority": "Medium"
}
""",
            "compatibility": """
Example Compatibility Test:
{
  "test_type": "compatibility",
  "test_title": "Verify GPS location accuracy across iOS and Android devices",
  "preconditions": "App installed on iOS 15+, Android 10+ devices, Location services enabled",
  "test_steps": [
    "Test on iPhone 12 (iOS 15): Enable location, perform restaurant search",
    "Record GPS accuracy and response time",
    "Test on Samsung Galaxy S21 (Android 12): Repeat same search",
    "Record GPS accuracy and response time",
    "Test on Pixel 6 (Android 13): Repeat search",
    "Compare accuracy across all devices",
    "Verify map rendering consistency"
  ],
  "expected_result": "GPS accuracy within 10 meters on all devices. Location updates within 2 seconds. Map renders correctly on both platforms. No platform-specific bugs.",
  "test_data": "Test location: 12.9716Â° N, 77.5946Â° E (Bangalore). Expected restaurants: 20-30 within 2km",
  "priority": "High"
}
""",
            "api": """
Example API Test:
{
  "test_type": "api",
  "test_title": "Verify restaurant search API response format and data integrity",
  "preconditions": "API endpoint accessible, Valid API key configured, Test data in database",
  "test_steps": [
    "Send GET request to /api/restaurants/search with valid coordinates",
    "Verify HTTP status code is 200",
    "Validate response JSON schema (id, name, lat, long, rating, cuisine)",
    "Check response time is < 500ms",
    "Verify pagination headers (page, limit, total_count)",
    "Test with invalid coordinates (HTTP 400 expected)",
    "Test without authentication (HTTP 401 expected)"
  ],
  "expected_result": "API returns 200 with valid JSON array. Schema matches specification. Response time < 500ms. Error codes correct for invalid requests. Pagination works correctly.",
  "test_data": "Endpoint: /api/restaurants/search?lat=12.9716&long=77.5946&radius=5",
  "priority": "High"
}
""",
            "data_integrity": """
Example Data Integrity Test:
{
  "test_type": "data_integrity",
  "test_title": "Verify restaurant rating calculation accuracy after user reviews",
  "preconditions": "Database accessible, Test restaurant exists with known ratings, Review system functional",
  "test_steps": [
    "Query restaurant 'Test Restaurant' current rating (3.5 stars, 10 reviews)",
    "Submit new 5-star review through app",
    "Wait for database sync (max 5 seconds)",
    "Query restaurant rating from database",
    "Calculate expected rating: (3.5*10 + 5*1)/11 = 3.64",
    "Verify displayed rating matches calculation",
    "Check review count incremented to 11",
    "Verify historical ratings preserved"
  ],
  "expected_result": "Rating updates to 3.64 stars. Review count shows 11. All previous reviews intact. Calculation accurate to 2 decimal places. No data loss during update.",
  "test_data": "Test Restaurant ID: rest_001, Current: 3.5 stars (10 reviews), New review: 5 stars",
  "priority": "High"
}
""",
            "reliability": """
Example Reliability Test:
{
  "test_type": "reliability",
  "test_title": "Verify app recovers location state after network interruption",
  "preconditions": "App running, Location tracking active, User mid-search, Network connected",
  "test_steps": [
    "Start restaurant search with location enabled",
    "During search, disable WiFi and mobile data",
    "Observe app behavior (should show offline mode)",
    "Wait 10 seconds",
    "Re-enable network connection",
    "Verify app auto-resumes search",
    "Check last known location preserved",
    "Verify search results populate correctly"
  ],
  "expected_result": "App shows 'Offline' indicator during network loss. No crash or freeze. Upon reconnection, app resumes within 3 seconds. Last location preserved. Search completes successfully with cached + new data.",
  "test_data": "Test location: 12.9716, 77.5946. Network interruption: 10 seconds",
  "priority": "Medium"
}
"""
        }
        
        example = few_shot_examples.get(test_type, "")
        
        prompt = f"""You are a senior QA engineer specializing in {test_type.upper()} testing for {domain} applications.

REQUIREMENT:
ID: {requirement['id']}
Title: {requirement['title']}
Description: {requirement['description']}
Rationale: {requirement.get('rationale', 'N/A')}

DOMAIN: {domain}

{example}

YOUR TASK:
Generate 2-3 detailed {test_type} test cases following the example format.

OUTPUT FORMAT (JSON array only):
[
  {{
    "test_type": "{test_type}",
    "test_title": "Specific descriptive title",
    "preconditions": "Detailed setup requirements",
    "test_steps": ["Detailed step 1", "Detailed step 2", "Detailed step 3", "Detailed step 4", "Detailed step 5"],
    "expected_result": "Precise, measurable expected outcome with specific values",
    "test_data": "Concrete test data with actual values",
    "priority": "High or Medium"
  }}
]

CRITICAL: Output ONLY valid JSON array. Be specific with test data and expected results.
"""
        return prompt


# ============================================
# FAST BATCH PROMPT GENERATOR (Phase 1)
# ============================================

class FastPromptGenerator:
    """Optimized prompt generator for batch processing"""
    
    @staticmethod
    def build_batch_prompt(requirement: Dict, domain: str, test_types: List[str]) -> str:
        """Generate ALL test types in ONE prompt for speed"""
        
        test_types_str = ", ".join(test_types)
        
        prompt = f"""Generate {len(test_types)} test cases (one per type) for this requirement.

REQUIREMENT:
ID: {requirement['id']}
Title: {requirement['title']}
Description: {requirement['description'][:200]}

DOMAIN: {domain}

GENERATE EXACTLY {len(test_types)} TEST CASES - ONE FOR EACH TYPE:
{test_types_str}

OUTPUT FORMAT (JSON array only, no markdown):
[
  {{
    "test_type": "positive",
    "test_title": "Brief clear title",
    "preconditions": "Setup requirements",
    "test_steps": ["Step 1", "Step 2", "Step 3", "Step 4"],
    "expected_result": "Expected outcome",
    "test_data": "Test data values",
    "priority": "High"
  }}
]

RULES:
- EXACTLY {len(test_types)} test cases
- 4-6 steps per test
- Be specific and concise
- Output ONLY JSON array
"""
        return prompt


# ============================================
# ENHANCED VALIDATION ENGINE
# ============================================

class HybridValidator:
    """Validation for both fast and comprehensive modes"""
    
    @staticmethod
    def validate(test_cases: List[Dict], requirement: Dict, is_comprehensive: bool = False) -> List[Dict]:
        """Validation with different standards for fast vs comprehensive"""
        validated = []
        
        for tc in test_cases:
            # Quick field check
            if not tc.get('test_title') or not tc.get('test_steps'):
                continue
            
            # Normalize steps
            if isinstance(tc['test_steps'], str):
                tc['test_steps'] = [s.strip() for s in tc['test_steps'].split('\n') if s.strip()]
            
            # Ensure minimum steps (higher for comprehensive)
            min_steps = 5 if is_comprehensive else 3
            if len(tc['test_steps']) < min_steps:
                while len(tc['test_steps']) < min_steps:
                    tc['test_steps'].append("Execute action and verify result")
            
            # Add requirement ID
            tc['requirement_id'] = requirement['id']
            
            # Normalize priority
            if tc.get('priority') not in ['High', 'Medium', 'Low']:
                tc['priority'] = 'High' if tc.get('test_type') in ['positive', 'negative', 'security'] else 'Medium'
            
            validated.append(tc)
        
        return validated


# ============================================
# HYBRID GENERATION ENGINE
# ============================================

class HybridTestGenerationEngine:
    """Dual-mode test generation: Fast batch + Comprehensive deep-dive"""
    
    def __init__(self, model_name: str = "qwen2.5-coder:7b-instruct"):
        self.model_name = model_name
        self.fast_prompt_gen = FastPromptGenerator()
        self.comprehensive_prompt_gen = ComprehensivePromptGenerator()
        self.validator = HybridValidator()
        self.test_counter = 1
        
        # Performance settings
        self.max_workers = 3  # Parallel LLM calls
        
        print(f"\n{'='*80}")
        print(f"âš¡ HYBRID TEST GENERATION ENGINE")
        print(f"{'='*80}")
        print(f"Model: {model_name}")
        print(f"Phase 1: Fast batch (6 types) - Parallel processing")
        print(f"Phase 2: Comprehensive (5 types) - Deep generation for critical reqs")
        print(f"{'='*80}\n")
        
        # Verify Ollama and keep model loaded
        try:
            ollama.list()
            ollama.generate(model=self.model_name, prompt="test", options={'num_predict': 1})
            print("âœ… Model loaded and ready\n")
        except Exception as e:
            print(f"âš ï¸ Ollama issue: {e}\n")
    
    def _call_llm(self, prompt: str, is_comprehensive: bool = False) -> Optional[str]:
        """Call LLM with different settings for fast vs comprehensive"""
        try:
            response = ollama.generate(
                model=self.model_name,
                prompt=prompt,
                options={
                    'temperature': 0.3 if is_comprehensive else 0.2,
                    'top_p': 0.9,
                    'num_predict': 5000 if is_comprehensive else 3000,
                    'repeat_penalty': 1.1
                }
            )
            return response['response']
        except Exception as e:
            print(f"  âš ï¸ LLM error: {e}")
            return None
    
    def _parse_json(self, response: str) -> List[Dict]:
        """Parse JSON from LLM response"""
        if not response:
            return []
        
        try:
            cleaned = response.strip()
            if cleaned.startswith('```'):
                lines = cleaned.split('\n')
                cleaned = '\n'.join(line for line in lines if not line.strip().startswith('```'))
            
            start = cleaned.find('[')
            end = cleaned.rfind(']') + 1
            
            if start == -1 or end == 0:
                return []
            
            json_str = cleaned[start:end]
            return json.loads(json_str)
        except:
            return []
    
    def _generate_fallback(self, requirement: Dict, test_type: str) -> Dict:
        """Quick fallback test case"""
        return {
            "test_type": test_type,
            "test_title": f"{test_type.capitalize()} test for {requirement['title'][:50]}",
            "preconditions": "Application ready, user logged in",
            "test_steps": [
                f"Navigate to {requirement['title']}",
                "Execute test action",
                "Input test data",
                "Verify result"
            ],
            "expected_result": requirement.get('rationale', 'Feature works as expected'),
            "test_data": "test_input",
            "priority": "Medium"
        }
    
    def generate_fast_batch(self, requirement: Dict, domain: str, test_types: List[str]) -> List[TestCase]:
        """Phase 1: Fast batch generation"""
        
        prompt = self.fast_prompt_gen.build_batch_prompt(requirement, domain, test_types)
        response = self._call_llm(prompt, is_comprehensive=False)
        generated = self._parse_json(response)
        
        # Fill missing types with fallback
        generated_types = set(tc.get('test_type') for tc in generated)
        for test_type in test_types:
            if test_type not in generated_types:
                generated.append(self._generate_fallback(requirement, test_type))
        
        validated = self.validator.validate(generated, requirement, is_comprehensive=False)
        
        # Convert to TestCase objects
        test_cases = []
        for tc in validated:
            test_case = TestCase(
                test_id=f"TC_{requirement['id']}_{self.test_counter:03d}",
                requirement_id=tc['requirement_id'],
                test_type=tc.get('test_type', 'positive'),
                test_title=tc['test_title'],
                preconditions=tc.get('preconditions', 'System ready'),
                test_steps=tc['test_steps'],
                expected_result=tc['expected_result'],
                test_data=tc.get('test_data', ''),
                priority=tc.get('priority', 'Medium'),
                generation_phase='fast_batch'
            )
            test_cases.append(test_case)
            self.test_counter += 1
        
        return test_cases
    
    def generate_comprehensive(self, requirement: Dict, domain: str, test_types: List[str]) -> List[TestCase]:
        """Phase 2: Comprehensive deep generation"""
        
        all_test_cases = []
        
        for test_type in test_types:
            prompt = self.comprehensive_prompt_gen.build_deep_prompt(requirement, domain, test_type)
            response = self._call_llm(prompt, is_comprehensive=True)
            generated = self._parse_json(response)
            
            if not generated:
                generated = [self._generate_fallback(requirement, test_type)]
            
            validated = self.validator.validate(generated, requirement, is_comprehensive=True)
            
            for tc in validated:
                test_case = TestCase(
                    test_id=f"TC_{requirement['id']}_{self.test_counter:03d}",
                    requirement_id=tc['requirement_id'],
                    test_type=tc.get('test_type', test_type),
                    test_title=tc['test_title'],
                    preconditions=tc.get('preconditions', 'System ready'),
                    test_steps=tc['test_steps'],
                    expected_result=tc['expected_result'],
                    test_data=tc.get('test_data', ''),
                    priority=tc.get('priority', 'Medium'),
                    generation_phase='comprehensive'
                )
                all_test_cases.append(test_case)
                self.test_counter += 1
        
        return all_test_cases
    
    def generate_parallel_fast(self, requirements: List[Dict], domain: str, test_types: List[str]) -> List[TestCase]:
        """Phase 1: Parallel fast batch processing"""
        
        all_test_cases = []
        total = len(requirements)
        
        print(f"ðŸš€ PHASE 1: Fast batch processing {total} requirements...")
        start_time = time.time()
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = {
                executor.submit(self.generate_fast_batch, req, domain, test_types): req 
                for req in requirements
            }
            
            completed = 0
            for future in as_completed(futures):
                req = futures[future]
                try:
                    test_cases = future.result()
                    all_test_cases.extend(test_cases)
                    completed += 1
                    
                    elapsed = time.time() - start_time
                    avg_time = elapsed / completed
                    remaining = (total - completed) * avg_time
                    
                    print(f"  âœ… {completed}/{total} | {req['id']} | "
                          f"{len(test_cases)} tests | ETA: {remaining/60:.1f}m")
                except Exception as e:
                    print(f"  âŒ {req['id']}: {e}")
                    completed += 1
        
        elapsed = time.time() - start_time
        print(f"\nâ±ï¸  Phase 1 completed in {elapsed/60:.1f} minutes")
        print(f"ðŸ“Š Generated {len(all_test_cases)} test cases\n")
        
        return all_test_cases
    
    def generate_sequential_comprehensive(self, requirements: List[Dict], domain: str, test_types: List[str]) -> List[TestCase]:
        """Phase 2: Sequential comprehensive processing for critical requirements"""
        
        all_test_cases = []
        total = len(requirements)
        
        print(f"ðŸ” PHASE 2: Comprehensive generation for {total} critical requirements...")
        start_time = time.time()
        
        for idx, req in enumerate(requirements, 1):
            print(f"\n  Processing {idx}/{total}: {req['id']} - {req['title'][:50]}...")
            
            try:
                test_cases = self.generate_comprehensive(req, domain, test_types)
                all_test_cases.extend(test_cases)
                
                elapsed = time.time() - start_time
                avg_time = elapsed / idx
                remaining = (total - idx) * avg_time
                
                print(f"  âœ… Generated {len(test_cases)} comprehensive tests | ETA: {remaining/60:.1f}m")
            except Exception as e:
                print(f"  âŒ Error: {e}")
        
        elapsed = time.time() - start_time
        print(f"\nâ±ï¸  Phase 2 completed in {elapsed/60:.1f} minutes")
        print(f"ðŸ“Š Generated {len(all_test_cases)} comprehensive test cases\n")
        
        return all_test_cases


# ============================================
# HYBRID ORCHESTRATOR
# ============================================

class HybridTestGenerator:
    """Main orchestrator for hybrid two-phase generation"""
    
    def __init__(
        self, 
        input_file: str,
        model_name: str = "qwen2.5-coder:7b-instruct"
    ):
        self.input_file = input_file
        self.engine = HybridTestGenerationEngine(model_name)
        self.data = None
        self.phase1_test_cases = []
        self.phase2_test_cases = []
        
        # Phase 1: Essential types (fast batch)
        self.phase1_types = ['positive', 'negative', 'edge', 'integration', 'performance', 'security']
        
        # Phase 2: Comprehensive types (deep generation)
        self.phase2_types = ['usability', 'compatibility', 'api', 'data_integrity', 'reliability']
        
        print(f"Phase 1 types: {self.phase1_types}")
        print(f"Phase 2 types: {self.phase2_types}\n")
    
    def load_data(self):
        """Load input data"""
        with open(self.input_file, 'r', encoding='utf-8') as f:
            self.data = json.load(f)
        
        print(f"{'='*80}")
        print(f"ðŸ“„ LOADED DATA")
        print(f"{'='*80}")
        print(f"Total Requirements: {self.data['metadata']['total_requirements']}")
        print(f"Chunks: {self.data['metadata']['total_chunks']}")
        print(f"Domain: {self.data['domain_classification']['primary_domain']}")
        print(f"{'='*80}\n")
    
    def identify_critical_requirements(self, top_n: int = 15) -> List[Dict]:
        """Identify critical requirements for Phase 2"""
        
        all_requirements = []
        for chunk in self.data['chunks']:
            all_requirements.extend(chunk['requirements'])
        
        # Priority heuristics:
        # 1. Requirements with dependencies (integration points)
        # 2. Requirements with many related requirements
        # 3. First N requirements (typically core features)
        
        critical_reqs = []
        
        # Add requirements with dependencies
        for req in all_requirements:
            if req.get('dependencies') and len(req['dependencies']) > 0:
                critical_reqs.append(req)
        
        # Add first N requirements (usually core features)
        for req in all_requirements[:top_n]:
            if req not in critical_reqs:
                critical_reqs.append(req)
        
        # Limit to top_n
        critical_reqs = critical_reqs[:top_n]
        
        print(f"{'='*80}")
        print(f"ðŸŽ¯ IDENTIFIED CRITICAL REQUIREMENTS")
        print(f"{'='*80}")
        print(f"Total critical requirements: {len(critical_reqs)}")
        for req in critical_reqs:
            print(f"  â€¢ {req['id']}: {req['title']}")
        print(f"{'='*80}\n")
        
        return critical_reqs
    
    def generate_phase1(self):
        """Phase 1: Fast batch for all requirements"""
        
        domain = self.data['domain_classification']['primary_domain']
        
        # Flatten all requirements
        all_requirements = []
        for chunk in self.data['chunks']:
            all_requirements.extend(chunk['requirements'])
        
        # Generate fast batch
        self.phase1_test_cases = self.engine.generate_parallel_fast(
            all_requirements,
            domain,
            self.phase1_types
        )
    
    def generate_phase2(self, critical_requirements: List[Dict]):
        """Phase 2: Comprehensive for critical requirements"""
        
        domain = self.data['domain_classification']['primary_domain']
        
        # Generate comprehensive tests
        self.phase2_test_cases = self.engine.generate_sequential_comprehensive(
            critical_requirements,
            domain,
            self.phase2_types
        )
    
    def save_results(self, output_prefix: str = "hybrid_test_cases"):
        """Save combined results"""
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        all_test_cases = self.phase1_test_cases + self.phase2_test_cases
        
        # 1. JSON
        json_data = {
            "metadata": {
                "generated": datetime.now().isoformat(),
                "model": self.engine.model_name,
                "total_test_cases": len(all_test_cases),
                "phase1_count": len(self.phase1_test_cases),
                "phase2_count": len(self.phase2_test_cases)
            },
            "phase1_test_cases": [asdict(tc) for tc in self.phase1_test_cases],
            "phase2_test_cases": [asdict(tc) for tc in self.phase2_test_cases]
        }
        
        json_file = f"{output_prefix}_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, indent=2)
        
        print(f"âœ… Saved JSON: {json_file}")
        
        # 2. Excel
        df = pd.DataFrame([asdict(tc) for tc in all_test_cases])
        df['test_steps'] = df['test_steps'].apply(
            lambda x: '\n'.join([f"{i+1}. {s}" for i, s in enumerate(x)])
        )
        
        excel_file = f"{output_prefix}_{timestamp}.xlsx"
        with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:
            # Overview
            overview = pd.DataFrame([{
                'Total Tests': len(all_test_cases),
                'Phase 1 (Fast)': len(self.phase1_test_cases),
                'Phase 2 (Comprehensive)': len(self.phase2_test_cases),
                'Positive': len([t for t in all_test_cases if t.test_type == 'positive']),
                'Negative': len([t for t in all_test_cases if t.test_type == 'negative']),
                'Edge': len([t for t in all_test_cases if t.test_type == 'edge']),
                'Integration': len([t for t in all_test_cases if t.test_type == 'integration']),
                'Performance': len([t for t in all_test_cases if t.test_type == 'performance']),
                'Security': len([t for t in all_test_cases if t.test_type == 'security']),
                'Usability': len([t for t in all_test_cases if t.test_type == 'usability']),
                'Compatibility': len([t for t in all_test_cases if t.test_type == 'compatibility']),
                'API': len([t for t in all_test_cases if t.test_type == 'api']),
                'Data Integrity': len([t for t in all_test_cases if t.test_type == 'data_integrity']),
                'Reliability': len([t for t in all_test_cases if t.test_type == 'reliability'])
            }])
            overview.to_excel(writer, sheet_name='Overview', index=False)
            
            # All tests
            df.to_excel(writer, sheet_name='All Tests', index=False)
            
            # Phase 1 tests
            phase1_df = df[df['generation_phase'] == 'fast_batch']
            if not phase1_df.empty:
                phase1_df.to_excel(writer, sheet_name='Phase 1 - Fast Batch', index=False)
            
            # Phase 2 tests
            phase2_df = df[df['generation_phase'] == 'comprehensive']
            if not phase2_df.empty:
                phase2_df.to_excel(writer, sheet_name='Phase 2 - Comprehensive', index=False)
            
            # By type
            all_types = self.phase1_types + self.phase2_types
            for test_type in all_types:
                type_df = df[df['test_type'] == test_type]
                if not type_df.empty:
                    type_df.to_excel(writer, sheet_name=test_type.capitalize()[:31], index=False)
        
        print(f"âœ… Saved Excel: {excel_file}")
        
        # 3. Summary
        summary_file = f"{output_prefix}_summary_{timestamp}.txt"
        with open(summary_file, 'w') as f:
            f.write("="*80 + "\n")
            f.write("HYBRID TEST GENERATION SUMMARY\n")
            f.write("="*80 + "\n\n")
            f.write(f"Generated: {datetime.now()}\n")
            f.write(f"Total Tests: {len(all_test_cases)}\n\n")
            
            f.write("PHASE 1 - FAST BATCH:\n")
            f.write(f"  Test Cases: {len(self.phase1_test_cases)}\n")
            f.write(f"  Types: {', '.join(self.phase1_types)}\n\n")
            
            f.write("PHASE 2 - COMPREHENSIVE:\n")
            f.write(f"  Test Cases: {len(self.phase2_test_cases)}\n")
            f.write(f"  Types: {', '.join(self.phase2_types)}\n\n")
            
            f.write("By Type:\n")
            for test_type in self.phase1_types + self.phase2_types:
                count = len([t for t in all_test_cases if t.test_type == test_type])
                f.write(f"  {test_type}: {count}\n")
            
            f.write("\nBy Priority:\n")
            for priority in ['High', 'Medium', 'Low']:
                count = len([t for t in all_test_cases if t.priority == priority])
                f.write(f"  {priority}: {count}\n")
        
        print(f"âœ… Saved Summary: {summary_file}")
        
        return json_file, excel_file, summary_file
    
    def show_final_stats(self):
        """Display final statistics"""
        all_test_cases = self.phase1_test_cases + self.phase2_test_cases
        
        print(f"\n{'='*80}")
        print(f"ðŸ“Š FINAL STATISTICS")
        print(f"{'='*80}")
        print(f"Total Test Cases: {len(all_test_cases)}")
        print(f"\nBy Phase:")
        print(f"  Phase 1 (Fast Batch): {len(self.phase1_test_cases)}")
        print(f"  Phase 2 (Comprehensive): {len(self.phase2_test_cases)}")
        print(f"\nBy Type:")
        for test_type in self.phase1_types + self.phase2_types:
            count = len([t for t in all_test_cases if t.test_type == test_type])
            print(f"  {test_type}: {count}")
        print(f"{'='*80}\n")


# ============================================
# MAIN EXECUTION
# ============================================

def main():
    """Hybrid two-phase execution"""
    
    # CONFIGURATION
    INPUT_FILE = "../03_Chunking_Domain_Understanding/chunked_requirements_with_domain.json"
    OUTPUT_PREFIX = "../04_AI_powered_TestCaseGeneration/hybrid_test_cases"
    MODEL = "qwen2.5-coder:7b-instruct"
    
    # Number of critical requirements for Phase 2
    NUM_CRITICAL_REQUIREMENTS = 15  # Adjust based on your needs
    
    print("="*80)
    print(" " * 20 + "âš¡ HYBRID TEST GENERATION")
    print("="*80)
    print("Phase 1: Fast batch for ALL requirements (6 types)")
    print(f"Phase 2: Comprehensive for TOP {NUM_CRITICAL_REQUIREMENTS} critical requirements (5 types)")
    print("="*80 + "\n")
    
    # Initialize
    generator = HybridTestGenerator(INPUT_FILE, model_name=MODEL)
    
    # Load
    print("[1/4] Loading data...")
    generator.load_data()
    
    # Identify critical requirements
    print("[2/4] Identifying critical requirements...")
    critical_reqs = generator.identify_critical_requirements(top_n=NUM_CRITICAL_REQUIREMENTS)
    
    # Phase 1: Fast batch
    print(f"[3/4] Phase 1: Fast batch generation...")
    phase1_start = time.time()
    generator.generate_phase1()
    phase1_time = time.time() - phase1_start
    
    # Phase 2: Comprehensive
    print(f"[4/4] Phase 2: Comprehensive generation...")
    phase2_start = time.time()
    generator.generate_phase2(critical_reqs)
    phase2_time = time.time() - phase2_start
    
    # Save
    print("\n[5/5] Saving results...")
    json_f, excel_f, summary_f = generator.save_results(OUTPUT_PREFIX)
    
    # Stats
    generator.show_final_stats()
    
    # Final summary
    total_time = phase1_time + phase2_time
    print("="*80)
    print("âœ… HYBRID GENERATION COMPLETE!")
    print("="*80)
    print(f"Phase 1 Time: {phase1_time/60:.1f} minutes")
    print(f"Phase 2 Time: {phase2_time/60:.1f} minutes")
    print(f"Total Time: {total_time/60:.1f} minutes")
    print(f"\nTotal Test Cases: {len(generator.phase1_test_cases) + len(generator.phase2_test_cases)}")
    print(f"  â€¢ Phase 1 (Fast): {len(generator.phase1_test_cases)}")
    print(f"  â€¢ Phase 2 (Comprehensive): {len(generator.phase2_test_cases)}")
    print(f"\nFiles:\n  â€¢ {json_f}\n  â€¢ {excel_f}\n  â€¢ {summary_f}")
    print("="*80 + "\n")


if __name__ == "__main__":
    main()
